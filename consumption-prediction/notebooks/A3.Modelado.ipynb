{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7333d54b",
   "metadata": {},
   "source": [
    "# Modelo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7799f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer # Added this import\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b62108",
   "metadata": {},
   "source": [
    "1. Leer los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65498b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Passenger_Count</th>\n",
       "      <th>Unit_Cost</th>\n",
       "      <th>haul</th>\n",
       "      <th>Origin_DOH</th>\n",
       "      <th>Origin_JFK</th>\n",
       "      <th>Origin_LHR</th>\n",
       "      <th>Origin_MEX</th>\n",
       "      <th>Origin_NRT</th>\n",
       "      <th>Origin_ZRH</th>\n",
       "      <th>Product_BRD001</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_SNK001</th>\n",
       "      <th>Percentage_Returned</th>\n",
       "      <th>day</th>\n",
       "      <th>flights</th>\n",
       "      <th>passengers</th>\n",
       "      <th>max capacity</th>\n",
       "      <th>Avg_Pass_Per_Flight_Day</th>\n",
       "      <th>Load_vs_Daily_Avg</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>272</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.327684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.274809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.180488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.482234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Passenger_Count  Unit_Cost  haul  Origin_DOH  Origin_JFK  Origin_LHR  \\\n",
       "0              272       0.35     2        True       False       False   \n",
       "1              272       0.80     2        True       False       False   \n",
       "2              272       0.75     2        True       False       False   \n",
       "3              272       0.45     2        True       False       False   \n",
       "4              272       0.50     2        True       False       False   \n",
       "\n",
       "   Origin_MEX  Origin_NRT  Origin_ZRH  Product_BRD001  ...  Product_SNK001  \\\n",
       "0       False       False       False            True  ...           False   \n",
       "1       False       False       False           False  ...           False   \n",
       "2       False       False       False           False  ...           False   \n",
       "3       False       False       False           False  ...           False   \n",
       "4       False       False       False           False  ...           False   \n",
       "\n",
       "   Percentage_Returned  day  flights  passengers  max capacity  \\\n",
       "0             0.327684  0.0      0.0         0.0           0.0   \n",
       "1             0.326531  0.0      0.0         0.0           0.0   \n",
       "2             0.274809  0.0      0.0         0.0           0.0   \n",
       "3             0.180488  0.0      0.0         0.0           0.0   \n",
       "4             0.482234  0.0      0.0         0.0           0.0   \n",
       "\n",
       "   Avg_Pass_Per_Flight_Day  Load_vs_Daily_Avg  Month  DayOfWeek  \n",
       "0                      0.0                0.0    NaN        NaN  \n",
       "1                      0.0                0.0    NaN        NaN  \n",
       "2                      0.0                0.0    NaN        NaN  \n",
       "3                      0.0                0.0    NaN        NaN  \n",
       "4                      0.0                0.0    9.0        4.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/consumption_features.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b786c",
   "metadata": {},
   "source": [
    "## Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608accc6",
   "metadata": {},
   "source": [
    "1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 2. Definición de X e Y ---\n",
    "TARGET_COLUMN = 'Percentage_Returned'\n",
    "Y = df[TARGET_COLUMN]\n",
    "X = df.drop(TARGET_COLUMN, axis=1)\n",
    "\n",
    "# Convertir booleanos a enteros (0/1) para compatibilidad de modelos\n",
    "bool_cols = X.select_dtypes(include='bool').columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Dividir Datos (mismo split que en A3) ---\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "# --- 3. Escalar Datos (para modelos que lo requieren) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_raw = scaler.fit_transform(X_train)\n",
    "X_test_scaled_raw = scaler.transform(X_test)\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_train_scaled = imputer.fit_transform(X_train_scaled_raw)\n",
    "X_test_scaled = imputer.transform(X_test_scaled_raw)\n",
    "\n",
    "# Convertir de nuevo a DataFrame para mantener nombres de columnas (opcional pero útil)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd57f8",
   "metadata": {},
   "source": [
    "## Modelos y Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831dd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Definir Modelos y Grillas de Hiperparámetros ---\n",
    "# (Inspirado en el notebook del Titanic, pero para Regresión)\n",
    "\n",
    "models_to_tune = [\n",
    "    {\n",
    "        'name': 'Ridge',\n",
    "        'model': Ridge(),\n",
    "        'params': {'alpha': [0.1, 1, 10, 100, 1000]},\n",
    "        'search_type': 'grid',\n",
    "        'scaled': True # Ridge se beneficia del escalado\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForestRegressor',\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': randint(5, 30),\n",
    "            'min_samples_leaf': randint(1, 10)\n",
    "        },\n",
    "        'search_type': 'random',\n",
    "        'scaled': False # Los árboles no lo necesitan\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVR',\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'C': uniform(0.1, 100),\n",
    "            'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        },\n",
    "        'search_type': 'random',\n",
    "        'scaled': True # SVR lo necesita\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBRegressor',\n",
    "        'model': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "        'params': {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 15),\n",
    "            'subsample': uniform(0.7, 0.3) # (0.7 a 1.0)\n",
    "        },\n",
    "        'search_type': 'random',\n",
    "        'scaled': False # Los árboles no lo necesitan\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710c24d",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a572fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Optimización de Modelos ---\n",
      "\n",
      "Optimizando Ridge...\n",
      "Mejores parámetros para Ridge: {'alpha': 1000}\n",
      "Mejor MAE (CV): 0.0757\n",
      "\n",
      "Optimizando RandomForestRegressor...\n",
      "Mejores parámetros para RandomForestRegressor: {'max_depth': 6, 'min_samples_leaf': 8, 'n_estimators': 393}\n",
      "Mejor MAE (CV): 0.0775\n",
      "\n",
      "Optimizando SVR...\n",
      "Mejores parámetros para SVR: {'C': np.float64(20.067378215835976), 'gamma': 0.01, 'kernel': 'poly'}\n",
      "Mejor MAE (CV): 0.0770\n",
      "\n",
      "Optimizando XGBRegressor...\n",
      "Mejores parámetros para XGBRegressor: {'learning_rate': np.float64(0.014116898859160489), 'max_depth': 4, 'n_estimators': 443, 'subsample': np.float64(0.9497327922401264)}\n",
      "Mejor MAE (CV): 0.0814\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "test_results = {}\n",
    "n_iter_random = 20 # Número de iteraciones para RandomizedSearch (ajústalo si quieres una búsqueda más larga)\n",
    "\n",
    "# --- 5. Bucle de Entrenamiento y Optimización ---\n",
    "print(\"\\n--- Iniciando Optimización de Modelos ---\")\n",
    "\n",
    "for config in models_to_tune:\n",
    "    name = config['name']\n",
    "    print(f\"\\nOptimizando {name}...\")\n",
    "    \n",
    "    # Elegir datos escalados o no escalados\n",
    "    X_train_data = X_train_scaled if config['scaled'] else X_train\n",
    "    \n",
    "    if config['search_type'] == 'grid':\n",
    "        search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "    else: # 'random'\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_distributions=config['params'],\n",
    "            n_iter=n_iter_random,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    search.fit(X_train_data, Y_train)\n",
    "    best_models[name] = search.best_estimator_\n",
    "    print(f\"Mejores parámetros para {name}: {search.best_params_}\")\n",
    "    print(f\"Mejor MAE (CV): {-search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a4f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación Final en el Test Set ---\n",
      "\n",
      "Ridge:\n",
      "  MAE (Test): 0.0837\n",
      "  R² (Test): 0.0139\n",
      "\n",
      "RandomForestRegressor:\n",
      "  MAE (Test): 0.0864\n",
      "  R² (Test): -0.0540\n",
      "\n",
      "SVR:\n",
      "  MAE (Test): 0.0828\n",
      "  R² (Test): -0.0213\n",
      "\n",
      "XGBRegressor:\n",
      "  MAE (Test): 0.0930\n",
      "  R² (Test): -0.2925\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Evaluación Final en el Test Set ---\n",
    "print(\"\\n--- Evaluación Final en el Test Set ---\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    \n",
    "    # Elegir datos de test escalados o no escalados\n",
    "    X_test_data = X_test_scaled if models_to_tune[next(i for i, d in enumerate(models_to_tune) if d['name'] == name)]['scaled'] else X_test\n",
    "    \n",
    "    Y_pred = model.predict(X_test_data)\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    test_results[name] = {'mae': mae, 'r2': r2}\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MAE (Test): {mae:.4f}\")\n",
    "    print(f\"  R² (Test): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a90470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- --- --- --- ---\n",
      "El mejor modelo es: SVR con un MAE de 0.0828\n",
      "El MAE está a 0.0628 puntos del objetivo (< 0.02).\n",
      "\n",
      "Guardando modelos en el directorio 'models'...\n",
      "Modelos guardados:\n",
      "- models/ridge_best.pkl\n",
      "- models/randomforestregressor_best.pkl\n",
      "- models/svr_best.pkl\n",
      "- models/xgbregressor_best.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Encontrar y Guardar el Mejor Modelo ---\n",
    "best_model_name = min(test_results, key=lambda k: test_results[k]['mae'])\n",
    "best_mae = test_results[best_model_name]['mae']\n",
    "\n",
    "print(\"\\n--- --- --- --- ---\")\n",
    "print(f\"El mejor modelo es: {best_model_name} con un MAE de {best_mae:.4f}\")\n",
    "if best_mae < 0.02:\n",
    "    print(\"¡Felicidades! Se alcanzó el objetivo de MAE < 0.02.\")\n",
    "else:\n",
    "    print(f\"El MAE está a {best_mae - 0.02:.4f} puntos del objetivo (< 0.02).\")\n",
    "\n",
    "# Crear directorio para guardar modelos\n",
    "output_dir = 'models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\nGuardando modelos en el directorio '{output_dir}'...\")\n",
    "\n",
    "saved_models = []\n",
    "for name, model in best_models.items():\n",
    "    # Guardamos todos los modelos optimizados, no solo el mejor\n",
    "    filename = f\"{output_dir}/{name.lower().replace(' ', '_')}_best.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    saved_models.append(filename)\n",
    "\n",
    "print(\"Modelos guardados:\")\n",
    "for f in saved_models:\n",
    "    print(f\"- {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe9b6d",
   "metadata": {},
   "source": [
    "# Recommendations for Improving the Model\n",
    "\n",
    "This section includes changes to improve the model's performance and reduce error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51aa935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning with updated parameter grid...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=5, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=300; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=2, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_split=10, n_estimators=300; total time=   1.0s\n",
      "Best parameters found: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# --- Improved Data Handling ---\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Replace filling NaN with 0 by using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "\n",
    "# Update parameter grid to replace 'auto' with 'sqrt'\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']  # Removed 'auto'\n",
    "}\n",
    "\n",
    "# Reinitialize GridSearchCV with error_score='raise' to debug invalid combinations\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    error_score='raise'  # Raise errors for invalid parameter combinations\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"Starting hyperparameter tuning with updated parameter grid...\")\n",
    "rf_grid_search.fit(X_train, Y_train)\n",
    "print(\"Best parameters found:\", rf_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c7b3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.0991\n",
      "\n",
      "Feature Importances:\n",
      "Passenger_Count: 0.2460\n",
      "DayOfWeek: 0.1312\n",
      "Unit_Cost: 0.1199\n",
      "Month: 0.0497\n",
      "Origin_JFK: 0.0416\n",
      "Product_DRK023: 0.0385\n",
      "Product_DRK024: 0.0369\n",
      "haul: 0.0334\n",
      "Origin_NRT: 0.0291\n",
      "Origin_MEX: 0.0289\n",
      "Product_NUT030: 0.0281\n",
      "Product_COF200: 0.0272\n",
      "Product_BRD001: 0.0267\n",
      "Origin_DOH: 0.0261\n",
      "Origin_LHR: 0.0216\n",
      "Origin_ZRH: 0.0214\n",
      "Product_CRK075: 0.0202\n",
      "Product_SNK001: 0.0199\n",
      "Product_HTB110: 0.0190\n",
      "Product_JCE200: 0.0176\n",
      "Product_CHO050: 0.0169\n",
      "day: 0.0000\n",
      "max capacity: 0.0000\n",
      "Avg_Pass_Per_Flight_Day: 0.0000\n",
      "Load_vs_Daily_Avg: 0.0000\n",
      "passengers: 0.0000\n",
      "flights: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Use the best model\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Additional Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict using the best model\n",
    "Y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "# Some sklearn versions don't accept the `squared` kwarg, so compute RMSE manually\n",
    "rf_rmse = np.sqrt(mean_squared_error(Y_test, Y_pred_best_rf))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rf_rmse:.4f}\")\n",
    "\n",
    "# Feature Importance\n",
    "importances = best_rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c10977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0098\n",
      "Mean Absolute Error (MAE): 0.0871\n",
      "R² (Coefficient of Determination): -0.0936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred_best_rf)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "mae = mean_absolute_error(Y_test, Y_pred_best_rf)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred_best_rf)\n",
    "print(f\"R² (Coefficient of Determination): {r2:.4f}\")\n",
    "\n",
    "#antes\n",
    "#Error Absoluto Medio (MAE) en Test: 0.0924\n",
    "#R-cuadrado (R²) en Test: -0.3205"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
